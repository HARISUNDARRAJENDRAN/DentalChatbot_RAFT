{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“š Dental Books Scraper & Data Processing (FAST VERSION)\n",
        "\n",
        "This notebook:\n",
        "1. Mounts Google Drive to access your dental books folder\n",
        "2. Scrapes ALL dental books from the library website (all categories) **with parallel downloads**\n",
        "3. Processes all PDFs for the RAFT pipeline\n",
        "\n",
        "**Google Drive Folder:** MyDentalBooks\n",
        "\n",
        "**Library URL:** http://43.230.198.52/lib/book/\n",
        "\n",
        "**Speed:** ~10x faster with parallel downloads (10 concurrent connections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q requests beautifulsoup4 tqdm PyMuPDF lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Your dental books folder\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/MyDentalBooks\"\n",
        "\n",
        "# Verify folder exists\n",
        "import os\n",
        "if os.path.exists(DRIVE_FOLDER):\n",
        "    existing_books = [f for f in os.listdir(DRIVE_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    print(f\"âœ“ Found MyDentalBooks folder with {len(existing_books)} existing PDF files\")\n",
        "else:\n",
        "    os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "    existing_books = []\n",
        "    print(f\"Created folder: {DRIVE_FOLDER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fast Parallel Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, unquote, quote\n",
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "@dataclass\n",
        "class BookInfo:\n",
        "    \"\"\"Information about a book.\"\"\"\n",
        "    title: str\n",
        "    url: str\n",
        "    category: str\n",
        "    filename: str\n",
        "\n",
        "\n",
        "class FastDentalLibraryScraper:\n",
        "    \"\"\"\n",
        "    Fast parallel scraper for the dental library.\n",
        "    Uses multiple threads for concurrent downloads.\n",
        "    \"\"\"\n",
        "    \n",
        "    BASE_URL = \"http://43.230.198.52/lib/book/\"\n",
        "    \n",
        "    # All known categories\n",
        "    CATEGORIES = [\n",
        "        \"Biochemistry\", \"Cancer\", \"Conservative Dentistry\",\n",
        "        \"Dental Anatomy & Oral Histology\", \"Dental Materials\",\n",
        "        \"Dermatology\", \"Dictionary\", \"Forensic\",\n",
        "        \"General Medicine\", \"General Pathology\", \"General Surgery\",\n",
        "        \"Genetics\", \"Hematology\", \"Human Anatomy\", \"Human Physiology\",\n",
        "        \"Immunology\", \"Implant Dentistry\", \"Microbiology\", \"Miscellaneous\",\n",
        "        \"Oral & Dental Anatomy\", \"Oral Medicine & Radiology\",\n",
        "        \"Oral Pathology\", \"Oral Surgery\", \"Orthodontics\",\n",
        "        \"Pedodontics\", \"Periodontics\", \"Pharmacology\",\n",
        "        \"Prosthodontics\", \"Public Health Dentistry\",\n",
        "    ]\n",
        "    \n",
        "    def __init__(self, output_dir: str, max_workers: int = 10, timeout: int = 60):\n",
        "        \"\"\"\n",
        "        Initialize fast scraper.\n",
        "        \n",
        "        Args:\n",
        "            output_dir: Directory to save downloaded books\n",
        "            max_workers: Number of parallel download threads (default: 10)\n",
        "            timeout: Request timeout in seconds\n",
        "        \"\"\"\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.max_workers = max_workers\n",
        "        self.timeout = timeout\n",
        "        \n",
        "        # Thread-local sessions for connection reuse\n",
        "        self._local = threading.local()\n",
        "        \n",
        "        # Statistics (thread-safe)\n",
        "        self._lock = threading.Lock()\n",
        "        self.stats = {\n",
        "            'discovered': 0,\n",
        "            'downloaded': 0,\n",
        "            'already_exists': 0,\n",
        "            'failed': 0\n",
        "        }\n",
        "    \n",
        "    def _get_session(self) -> requests.Session:\n",
        "        \"\"\"Get thread-local session.\"\"\"\n",
        "        if not hasattr(self._local, 'session'):\n",
        "            session = requests.Session()\n",
        "            session.headers.update({\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"*/*\",\n",
        "            })\n",
        "            # Connection pooling for speed\n",
        "            adapter = requests.adapters.HTTPAdapter(\n",
        "                pool_connections=20,\n",
        "                pool_maxsize=20,\n",
        "                max_retries=3\n",
        "            )\n",
        "            session.mount('http://', adapter)\n",
        "            self._local.session = session\n",
        "        return self._local.session\n",
        "    \n",
        "    def get_page(self, url: str) -> BeautifulSoup:\n",
        "        \"\"\"Fetch and parse a webpage.\"\"\"\n",
        "        try:\n",
        "            response = self._get_session().get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return BeautifulSoup(response.content, \"html.parser\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def discover_categories(self) -> List[str]:\n",
        "        \"\"\"Discover all category folders.\"\"\"\n",
        "        print(f\"Discovering categories from {self.BASE_URL}\")\n",
        "        \n",
        "        soup = self.get_page(self.BASE_URL)\n",
        "        if soup is None:\n",
        "            print(\"Using predefined categories\")\n",
        "            return self.CATEGORIES\n",
        "        \n",
        "        categories = []\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link[\"href\"]\n",
        "            if href in [\"../\", \"/\", \"#\"] or href.startswith(\"?\"):\n",
        "                continue\n",
        "            if href.endswith(\"/\"):\n",
        "                category_name = unquote(href.rstrip(\"/\"))\n",
        "                if category_name and not category_name.startswith(\".\"):\n",
        "                    categories.append(category_name)\n",
        "        \n",
        "        if categories:\n",
        "            print(f\"Found {len(categories)} categories\")\n",
        "            return categories\n",
        "        return self.CATEGORIES\n",
        "    \n",
        "    def discover_books_in_category(self, category: str) -> List[BookInfo]:\n",
        "        \"\"\"Discover all PDF books in a category.\"\"\"\n",
        "        books = []\n",
        "        category_url = urljoin(self.BASE_URL, quote(category) + \"/\")\n",
        "        \n",
        "        soup = self.get_page(category_url)\n",
        "        if soup is None:\n",
        "            return books\n",
        "        \n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link[\"href\"]\n",
        "            if href.lower().endswith(\".pdf\"):\n",
        "                full_url = urljoin(category_url, href)\n",
        "                filename = unquote(href)\n",
        "                title = self._clean_title(Path(filename).stem)\n",
        "                \n",
        "                books.append(BookInfo(\n",
        "                    title=title,\n",
        "                    url=full_url,\n",
        "                    category=category,\n",
        "                    filename=filename\n",
        "                ))\n",
        "        \n",
        "        return books\n",
        "    \n",
        "    def discover_all_books(self, categories: List[str] = None) -> List[BookInfo]:\n",
        "        \"\"\"Discover all books from all categories (parallel).\"\"\"\n",
        "        if categories is None:\n",
        "            categories = self.discover_categories()\n",
        "        \n",
        "        all_books = []\n",
        "        print(f\"\\nScanning {len(categories)} categories in parallel...\\n\")\n",
        "        \n",
        "        # Parallel category scanning\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = {executor.submit(self.discover_books_in_category, cat): cat \n",
        "                      for cat in categories}\n",
        "            \n",
        "            for future in tqdm(as_completed(futures), total=len(categories), desc=\"Scanning\"):\n",
        "                category = futures[future]\n",
        "                try:\n",
        "                    books = future.result()\n",
        "                    all_books.extend(books)\n",
        "                    if books:\n",
        "                        tqdm.write(f\"  {category}: {len(books)} books\")\n",
        "                except Exception as e:\n",
        "                    tqdm.write(f\"  {category}: Error - {e}\")\n",
        "        \n",
        "        self.stats['discovered'] = len(all_books)\n",
        "        print(f\"\\nâœ“ Discovered {len(all_books)} total books\")\n",
        "        return all_books\n",
        "    \n",
        "    def _clean_title(self, title: str) -> str:\n",
        "        \"\"\"Clean up book title.\"\"\"\n",
        "        title = re.sub(r\"^\\d+[\\._-]?\\s*\", \"\", title)\n",
        "        title = re.sub(r\"[\\._-]+\", \" \", title)\n",
        "        title = re.sub(r\"\\s+\", \" \", title)\n",
        "        return title.strip()[:200]\n",
        "    \n",
        "    def _get_safe_filename(self, book: BookInfo) -> str:\n",
        "        \"\"\"Create safe filename.\"\"\"\n",
        "        safe_category = re.sub(r'[<>:\"/\\\\|?*&]', \"_\", book.category)\n",
        "        safe_title = re.sub(r'[<>:\"/\\\\|?*&]', \"_\", book.title)\n",
        "        if len(safe_title) > 150:\n",
        "            safe_title = safe_title[:150]\n",
        "        return f\"[{safe_category}] {safe_title}.pdf\"\n",
        "    \n",
        "    def _download_single(self, book: BookInfo) -> Tuple[BookInfo, bool, str]:\n",
        "        \"\"\"Download a single book (called by thread pool).\"\"\"\n",
        "        filename = self._get_safe_filename(book)\n",
        "        filepath = self.output_dir / filename\n",
        "        \n",
        "        # Skip if exists\n",
        "        if filepath.exists() and filepath.stat().st_size > 1000:\n",
        "            return book, True, \"exists\"\n",
        "        \n",
        "        try:\n",
        "            response = self._get_session().get(\n",
        "                book.url,\n",
        "                timeout=self.timeout,\n",
        "                stream=True\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # Verify it's a PDF\n",
        "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "            if \"html\" in content_type.lower():\n",
        "                return book, False, \"not_pdf\"\n",
        "            \n",
        "            # Download\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=32768):  # Larger chunks\n",
        "                    f.write(chunk)\n",
        "            \n",
        "            return book, True, \"downloaded\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            return book, False, str(e)[:50]\n",
        "    \n",
        "    def download_all_parallel(self, books: List[BookInfo]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Download all books in parallel.\n",
        "        \n",
        "        Args:\n",
        "            books: List of BookInfo objects\n",
        "            \n",
        "        Returns:\n",
        "            Statistics dictionary\n",
        "        \"\"\"\n",
        "        print(f\"\\nâš¡ Downloading {len(books)} books with {self.max_workers} parallel connections...\\n\")\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all download tasks\n",
        "            futures = {executor.submit(self._download_single, book): book for book in books}\n",
        "            \n",
        "            # Process as they complete\n",
        "            for future in tqdm(as_completed(futures), total=len(books), desc=\"Downloading\"):\n",
        "                try:\n",
        "                    book, success, status = future.result()\n",
        "                    \n",
        "                    with self._lock:\n",
        "                        if success:\n",
        "                            if status == \"exists\":\n",
        "                                self.stats['already_exists'] += 1\n",
        "                            else:\n",
        "                                self.stats['downloaded'] += 1\n",
        "                        else:\n",
        "                            self.stats['failed'] += 1\n",
        "                            if \"not_pdf\" not in status:\n",
        "                                tqdm.write(f\"  âœ— {book.title[:40]}... - {status}\")\n",
        "                                \n",
        "                except Exception as e:\n",
        "                    with self._lock:\n",
        "                        self.stats['failed'] += 1\n",
        "        \n",
        "        return self.stats\n",
        "    \n",
        "    def run(self, categories: List[str] = None) -> Dict[str, int]:\n",
        "        \"\"\"Run full scraping pipeline.\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Discover books\n",
        "        books = self.discover_all_books(categories)\n",
        "        \n",
        "        if not books:\n",
        "            print(\"No books found\")\n",
        "            return self.stats\n",
        "        \n",
        "        # Download in parallel\n",
        "        self.download_all_parallel(books)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DOWNLOAD SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Discovered:      {self.stats['discovered']}\")\n",
        "        print(f\"Downloaded:      {self.stats['downloaded']}\")\n",
        "        print(f\"Already existed: {self.stats['already_exists']}\")\n",
        "        print(f\"Failed:          {self.stats['failed']}\")\n",
        "        print(f\"Time elapsed:    {elapsed/60:.1f} minutes\")\n",
        "        print(f\"Speed:           {len(books)/elapsed:.1f} books/second\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        return self.stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run the Fast Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize FAST scraper with 10 parallel downloads\n",
        "scraper = FastDentalLibraryScraper(\n",
        "    output_dir=DRIVE_FOLDER,\n",
        "    max_workers=10,  # 10 parallel downloads (adjust if needed)\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "print(f\"Output folder: {DRIVE_FOLDER}\")\n",
        "print(f\"Parallel downloads: {scraper.max_workers}\")\n",
        "print(f\"Expected speedup: ~{scraper.max_workers}x faster\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the scraper!\n",
        "stats = scraper.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Scrape specific categories only\n",
        "# selected_categories = [\n",
        "#     \"Conservative Dentistry\",\n",
        "#     \"Orthodontics\",\n",
        "#     \"Prosthodontics\",\n",
        "#     \"Periodontics\",\n",
        "# ]\n",
        "# stats = scraper.run(categories=selected_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify Downloaded Books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count all PDFs\n",
        "all_pdfs = [f for f in os.listdir(DRIVE_FOLDER) if f.lower().endswith('.pdf')]\n",
        "print(f\"Total dental books in Google Drive: {len(all_pdfs)}\")\n",
        "\n",
        "# Total size\n",
        "total_size = sum(os.path.getsize(os.path.join(DRIVE_FOLDER, f)) for f in all_pdfs)\n",
        "print(f\"Total size: {total_size / (1024**3):.2f} GB\")\n",
        "\n",
        "# Count by category\n",
        "print(\"\\nBooks by category:\")\n",
        "category_counts = {}\n",
        "for pdf in all_pdfs:\n",
        "    match = re.match(r'\\[([^\\]]+)\\]', pdf)\n",
        "    cat = match.group(1) if match else 'Uncategorized'\n",
        "    category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {cat}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract Text from All PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing\n",
        "\n",
        "def extract_pdf_text(pdf_path: str) -> list:\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        pages = []\n",
        "        filename = os.path.basename(pdf_path)\n",
        "        \n",
        "        # Extract category\n",
        "        match = re.match(r'\\[([^\\]]+)\\]', filename)\n",
        "        category = match.group(1) if match else \"Unknown\"\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text = page.get_text(\"text\")\n",
        "            text = \" \".join(text.split())\n",
        "            \n",
        "            if len(text) >= 100:\n",
        "                pages.append({\n",
        "                    \"text\": text,\n",
        "                    \"source\": filename,\n",
        "                    \"category\": category,\n",
        "                    \"page_number\": page_num + 1,\n",
        "                    \"total_pages\": len(doc)\n",
        "                })\n",
        "        \n",
        "        doc.close()\n",
        "        return pages\n",
        "    except Exception as e:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output directory\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/RAFT_dental_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, \"raw_pages.jsonl\")\n",
        "\n",
        "print(f\"Extracting text from {len(all_pdfs)} PDFs...\")\n",
        "print(f\"Output: {output_file}\")\n",
        "print()\n",
        "\n",
        "total_pages = 0\n",
        "processed = 0\n",
        "failed = 0\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for pdf_name in tqdm(all_pdfs, desc=\"Extracting\"):\n",
        "        pdf_path = os.path.join(DRIVE_FOLDER, pdf_name)\n",
        "        pages = extract_pdf_text(pdf_path)\n",
        "        \n",
        "        if pages:\n",
        "            for page in pages:\n",
        "                f.write(json.dumps(page, ensure_ascii=False) + \"\\n\")\n",
        "                total_pages += 1\n",
        "            processed += 1\n",
        "        else:\n",
        "            failed += 1\n",
        "\n",
        "print(f\"\\nâœ“ Done! Processed: {processed}, Failed: {failed}, Total pages: {total_pages}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Chunk Text for Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain-text-splitters tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "def count_tokens(text): return len(tokenizer.encode(text))\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2048, chunk_overlap=400,\n",
        "    length_function=count_tokens,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "print(\"Chunker ready: ~512 tokens per chunk, 100 token overlap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks_file = os.path.join(OUTPUT_DIR, \"chunks.jsonl\")\n",
        "\n",
        "total_chunks = 0\n",
        "with open(output_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
        "     open(chunks_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "    \n",
        "    for line in tqdm(fin, desc=\"Chunking\"):\n",
        "        page = json.loads(line)\n",
        "        if len(page[\"text\"]) < 100:\n",
        "            continue\n",
        "        \n",
        "        chunks = splitter.split_text(page[\"text\"])\n",
        "        \n",
        "        for idx, chunk_text in enumerate(chunks):\n",
        "            chunk = {\n",
        "                \"chunk_id\": f\"{page['source']}_{page['page_number']}_{idx}\",\n",
        "                \"text\": chunk_text,\n",
        "                \"source\": page[\"source\"],\n",
        "                \"category\": page.get(\"category\", \"Unknown\"),\n",
        "                \"page_number\": page[\"page_number\"],\n",
        "                \"chunk_index\": idx,\n",
        "                \"token_count\": count_tokens(chunk_text)\n",
        "            }\n",
        "            fout.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
        "            total_chunks += 1\n",
        "\n",
        "print(f\"\\nâœ“ Created {total_chunks} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_size = os.path.getsize(output_file) / (1024**2) if os.path.exists(output_file) else 0\n",
        "chunks_size = os.path.getsize(chunks_file) / (1024**2) if os.path.exists(chunks_file) else 0\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ðŸ“ Books: {len(all_pdfs)} PDFs ({total_size/(1024**3):.2f} GB)\")\n",
        "print(f\"ðŸ“„ Pages: {total_pages} ({raw_size:.1f} MB)\")\n",
        "print(f\"ðŸ§© Chunks: {total_chunks} ({chunks_size:.1f} MB)\")\n",
        "print(f\"ðŸ“‚ Output: {OUTPUT_DIR}\")\n",
        "print(\"=\"*60)\n",
        "print(\"Next: Run 02_qa_generation.ipynb\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
