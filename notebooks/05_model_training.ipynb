{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAFT Fine-Tuning for Dental Chatbot\n",
        "\n",
        "This notebook fine-tunes Llama 3.1 8B on the RAFT dental dataset using QLoRA.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with T4 GPU (free) or A100 (Pro)\n",
        "- HuggingFace account for model access\n",
        "- RAFT dataset prepared (train.jsonl, val.jsonl)\n",
        "\n",
        "**Expected Training Time:**\n",
        "- T4 (16GB): ~3-5 hours for 15K examples\n",
        "- A100 (40GB): ~1-2 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers accelerate bitsandbytes\n",
        "!pip install -q peft trl datasets\n",
        "!pip install -q wandb huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Login to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "# Make sure you have accepted Llama 3.1 license at:\n",
        "# https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load RAFT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_raft_dataset(file_path):\n",
        "    \"\"\"Load RAFT dataset from JSONL file.\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "# Upload your dataset files to Colab or mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "TRAIN_PATH = \"train.jsonl\"  # Update path\n",
        "VAL_PATH = \"val.jsonl\"      # Update path\n",
        "\n",
        "train_dataset = load_raft_dataset(TRAIN_PATH)\n",
        "val_dataset = load_raft_dataset(VAL_PATH)\n",
        "\n",
        "print(f\"Train examples: {len(train_dataset)}\")\n",
        "print(f\"Val examples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a training example\n",
        "example = train_dataset[0]\n",
        "print(\"Question:\", example['question'][:100])\n",
        "print(\"\\nContext docs:\", len(example['context']))\n",
        "print(\"\\nAnswer:\", example['answer'][:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Base Model with Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# QLoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                          # Low-rank dimension\n",
        "    lora_alpha=32,                 # Scaling factor (typically 2Ã—r)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"       # FFN\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Format Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_raft_prompt(example):\n",
        "    \"\"\"\n",
        "    Format RAFT example into Llama 3.1 Instruct format.\n",
        "    \"\"\"\n",
        "    # Format context documents\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(example['context']):\n",
        "        context_parts.append(\n",
        "            f\"Document {i+1} ({doc['source']}, p.{doc['page_number']}):\\n{doc['content']}\"\n",
        "        )\n",
        "    context_str = \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    # Llama 3.1 Instruct format\n",
        "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a dental education assistant. Answer questions using the provided documents. Cite sources using ##begin_quote## and ##end_quote## markers. If documents don't contain relevant information, say so clearly.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Question: {example['question']}\n",
        "\n",
        "Documents:\n",
        "{context_str}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{example['answer']}<|eot_id|>\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Test formatting\n",
        "sample_prompt = format_raft_prompt(train_dataset[0])\n",
        "print(sample_prompt[:500])\n",
        "print(\"...\")\n",
        "print(sample_prompt[-300:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check token count\n",
        "tokens = tokenizer(sample_prompt, return_tensors=\"pt\")\n",
        "print(f\"Sample prompt tokens: {tokens['input_ids'].shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints/llama-3.1-8b-dental-raft\",\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,        # Keep low for T4\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,        # Effective batch size = 8\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    fp16=False,\n",
        "    bf16=True,                            # Use bf16 on Ampere+ GPUs\n",
        "    \n",
        "    # Logging and saving\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Evaluation\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    \n",
        "    # Memory management\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    \n",
        "    # Misc\n",
        "    report_to=\"wandb\",                    # Optional: track with W&B\n",
        "    run_name=\"dental-raft-llama3.1-8b\",\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Initialize Weights & Biases\n",
        "import wandb\n",
        "\n",
        "# wandb.login()  # Uncomment if you want to track with W&B\n",
        "# wandb.init(project=\"dental-raft\", name=\"llama3.1-8b-qlora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=format_raft_prompt,\n",
        "    max_seq_length=2048,                  # Adjust based on GPU memory\n",
        "    packing=False,                        # Don't pack multiple examples\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check memory before training\n",
        "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "trainer.save_model(\"./final_model\")\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(question, context_docs):\n",
        "    \"\"\"\n",
        "    Generate answer using fine-tuned model.\n",
        "    \"\"\"\n",
        "    # Format context\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(context_docs):\n",
        "        context_parts.append(\n",
        "            f\"Document {i+1} ({doc['source']}, p.{doc['page_number']}):\\n{doc['content']}\"\n",
        "        )\n",
        "    context_str = \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a dental education assistant. Answer questions using the provided documents. Cite sources using ##begin_quote## and ##end_quote## markers.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Documents:\n",
        "{context_str}<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract assistant response\n",
        "    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
        "        answer = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
        "    else:\n",
        "        answer = response[len(prompt):]\n",
        "    \n",
        "    return answer.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample from validation set\n",
        "test_example = val_dataset[0]\n",
        "\n",
        "print(\"Question:\", test_example['question'])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "answer = generate_answer(\n",
        "    test_example['question'],\n",
        "    test_example['context']\n",
        ")\n",
        "\n",
        "print(\"Generated Answer:\")\n",
        "print(answer)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"Ground Truth:\")\n",
        "print(test_example['answer'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Push to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA weights with base model for easier deployment\n",
        "from peft import PeftModel\n",
        "\n",
        "# If you saved and want to reload:\n",
        "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, ...)\n",
        "# model = PeftModel.from_pretrained(model, \"./final_model\")\n",
        "\n",
        "# Merge weights\n",
        "merged_model = model.merge_and_unload()\n",
        "print(\"Weights merged!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push to HuggingFace Hub\n",
        "HF_USERNAME = \"your-username\"  # Change this!\n",
        "MODEL_REPO = f\"{HF_USERNAME}/llama-3.1-8b-dental-raft\"\n",
        "\n",
        "# Push model\n",
        "merged_model.push_to_hub(MODEL_REPO, private=True)\n",
        "tokenizer.push_to_hub(MODEL_REPO, private=True)\n",
        "\n",
        "print(f\"Model pushed to: https://huggingface.co/{MODEL_REPO}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "import gc\n",
        "\n",
        "del model\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Download the FAISS index** from your data processing step\n",
        "2. **Deploy to HuggingFace Spaces** with ZeroGPU:\n",
        "   - Create a new Space with Gradio SDK\n",
        "   - Upload `app/app.py` and `app/requirements.txt`\n",
        "   - Upload your FAISS index files\n",
        "   - Update `MODEL_NAME` in app.py to point to your pushed model\n",
        "\n",
        "3. **Test the deployment** and iterate on the prompts if needed"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
