{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ RAFT Dataset Creation\n",
    "\n",
    "This notebook creates the final RAFT (Retrieval Augmented Fine-Tuning) dataset.\n",
    "\n",
    "**RAFT Format:**\n",
    "- Each example has 1 oracle document (contains answer) + 4 distractor documents\n",
    "- 80% examples: oracle + distractors\n",
    "- 20% examples: only distractors (teach model to say \"I don't know\")\n",
    "\n",
    "**Input:** `qa_pairs.jsonl`, `chunks.jsonl`\n",
    "**Output:** `train.jsonl`, `val.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# For local setup, use pip in terminal:\n",
    "# pip install sentence-transformers faiss-cpu tqdm torch\n",
    "\n",
    "# Or uncomment below to install in notebook:\n",
    "# !uv pip install -q sentence-transformers faiss-cpu tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOCAL SETUP (Google Drive Desktop) =====\n",
    "import os\n",
    "\n",
    "# Google Drive Desktop path\n",
    "DATA_DIR = \"G:/My Drive/RAFT_dental_data\"\n",
    "\n",
    "CHUNKS_FILE = f\"{DATA_DIR}/chunks.jsonl\"\n",
    "QA_FILE = f\"{DATA_DIR}/qa_pairs.jsonl\"\n",
    "OUTPUT_DIR = f\"{DATA_DIR}/raft_dataset\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify inputs\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "for f in [CHUNKS_FILE, QA_FILE]:\n",
    "    if os.path.exists(f):\n",
    "        size_mb = os.path.getsize(f) / (1024**2)\n",
    "        print(f\"âœ“ Found {os.path.basename(f)} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing {os.path.basename(f)} - check your path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load chunks (these are smaller, ~100MB)\n",
    "chunks = []\n",
    "chunk_by_id = {}\n",
    "with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunk = json.loads(line)\n",
    "        chunks.append(chunk)\n",
    "        chunk_by_id[chunk['chunk_id']] = chunk\n",
    "\n",
    "print(f\"âœ“ Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Count Q&A pairs without loading into memory\n",
    "print(\"Counting Q&A pairs...\")\n",
    "qa_count = 0\n",
    "with open(QA_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        qa_count += 1\n",
    "print(f\"âœ“ Found {qa_count} Q&A pairs (will stream during processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Embedding Index for Distractor Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use FASTER embedding model (still good quality, 5x faster)\n",
    "# Options:\n",
    "# - 'all-MiniLM-L6-v2' = FASTEST (384D, great quality)\n",
    "# - 'pritamdeka/S-PubMedBert-MS-MARCO' = Medical domain but slower\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "# embed_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO', device=device)  # Use this for medical domain\n",
    "\n",
    "print(f\"âœ“ Model loaded: {embed_model.get_sentence_embedding_dimension()}D embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings - MAXIMUM SPEED\n",
    "print(\"Generating chunk embeddings (MAX GPU speed)...\")\n",
    "\n",
    "chunk_texts = [c['text'] for c in chunks]\n",
    "\n",
    "# Maximum speed settings\n",
    "import torch\n",
    "with torch.cuda.amp.autocast():  # Use fp16 for speed\n",
    "    chunk_embeddings = embed_model.encode(\n",
    "        chunk_texts,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=512,  # Even larger batch\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Embeddings shape: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index (embeddings already normalized)\n",
    "print(\"Building FAISS index...\")\n",
    "\n",
    "dimension = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine for normalized vectors\n",
    "index.add(chunk_embeddings.astype(np.float32))\n",
    "\n",
    "print(f\"âœ“ FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAFT Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Iterator\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class RAFTConfig:\n",
    "    \"\"\"Configuration for RAFT dataset creation.\"\"\"\n",
    "    num_distractors: int = 4\n",
    "    oracle_ratio: float = 0.8\n",
    "    max_context_length: int = 3000\n",
    "    train_ratio: float = 0.9\n",
    "\n",
    "\n",
    "class RAFTDatasetBuilder:\n",
    "    \"\"\"Build RAFT dataset with SEMANTIC distractors - OPTIMIZED.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        chunk_embeddings: np.ndarray,\n",
    "        faiss_index: faiss.Index,\n",
    "        embed_model: SentenceTransformer,\n",
    "        config: RAFTConfig = None\n",
    "    ):\n",
    "        self.chunks = chunks\n",
    "        self.chunk_embeddings = chunk_embeddings\n",
    "        self.index = faiss_index\n",
    "        self.embed_model = embed_model\n",
    "        self.config = config or RAFTConfig()\n",
    "        self.chunk_by_id = {c['chunk_id']: i for i, c in enumerate(chunks)}\n",
    "    \n",
    "    def find_distractors_batch(self, questions: List[str], oracle_indices: List[int], k: int = 6) -> List[List[int]]:\n",
    "        \"\"\"Find semantic distractors for batch of questions.\"\"\"\n",
    "        # Batch encode on GPU\n",
    "        q_embeddings = self.embed_model.encode(\n",
    "            questions, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=len(questions)  # Process all at once\n",
    "        )\n",
    "        faiss.normalize_L2(q_embeddings)\n",
    "        \n",
    "        # Batch FAISS search\n",
    "        _, all_indices = self.index.search(q_embeddings.astype(np.float32), k + 5)\n",
    "        \n",
    "        results = []\n",
    "        for i, indices in enumerate(all_indices):\n",
    "            oracle_idx = oracle_indices[i]\n",
    "            distractors = [int(idx) for idx in indices if idx != oracle_idx]\n",
    "            results.append(distractors[:k])\n",
    "        return results\n",
    "    \n",
    "    def format_context_document(self, chunk: Dict, is_oracle: bool = False) -> Dict:\n",
    "        return {\n",
    "            \"content\": chunk['text'],\n",
    "            \"source\": chunk.get('source', 'Unknown'),\n",
    "            \"page_number\": chunk.get('page_number', 0),\n",
    "            \"category\": chunk.get('category', 'Unknown'),\n",
    "            \"is_oracle\": is_oracle\n",
    "        }\n",
    "    \n",
    "    def _create_no_oracle_answer(self, question: str) -> str:\n",
    "        return (\n",
    "            \"Based on the provided documents, I cannot find sufficient information \"\n",
    "            \"to accurately answer this question. The documents do not contain \"\n",
    "            \"relevant content about this specific topic.\"\n",
    "        )\n",
    "    \n",
    "    def stream_qa_pairs(self, qa_file: str) -> Iterator[Dict]:\n",
    "        with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    yield json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    def build_dataset_streaming(\n",
    "        self,\n",
    "        qa_file: str,\n",
    "        output_dir: str,\n",
    "        total_count: int,\n",
    "        batch_size: int = 1000\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"Build RAFT dataset with semantic distractors - OPTIMIZED batching.\"\"\"\n",
    "        print(f\"Building RAFT dataset from {total_count} Q&A pairs (semantic distractors)...\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        \n",
    "        train_path = os.path.join(output_dir, \"train.jsonl\")\n",
    "        val_path = os.path.join(output_dir, \"val.jsonl\")\n",
    "        \n",
    "        total_examples = 0\n",
    "        failed = 0\n",
    "        with_oracle_count = 0\n",
    "        without_oracle_count = 0\n",
    "        \n",
    "        split_idx = int(total_count * self.config.train_ratio)\n",
    "        \n",
    "        train_f = open(train_path, 'w', encoding='utf-8')\n",
    "        val_f = open(val_path, 'w', encoding='utf-8')\n",
    "        \n",
    "        try:\n",
    "            batch = []\n",
    "            global_idx = 0\n",
    "            \n",
    "            for qa in tqdm(self.stream_qa_pairs(qa_file), total=total_count, desc=\"Processing\"):\n",
    "                batch.append((qa, global_idx))\n",
    "                global_idx += 1\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process batch\n",
    "                    results = self._process_batch_semantic(batch, split_idx)\n",
    "                    \n",
    "                    for example, is_train, success, has_oracle in results:\n",
    "                        if success:\n",
    "                            line = json.dumps(example, ensure_ascii=False) + \"\\n\"\n",
    "                            if is_train:\n",
    "                                train_f.write(line)\n",
    "                            else:\n",
    "                                val_f.write(line)\n",
    "                            total_examples += 1\n",
    "                            if has_oracle:\n",
    "                                with_oracle_count += 1\n",
    "                            else:\n",
    "                                without_oracle_count += 1\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                    \n",
    "                    batch = []\n",
    "                    \n",
    "                    if global_idx % 20000 == 0:\n",
    "                        train_f.flush()\n",
    "                        val_f.flush()\n",
    "            \n",
    "            # Process remaining\n",
    "            if batch:\n",
    "                results = self._process_batch_semantic(batch, split_idx)\n",
    "                for example, is_train, success, has_oracle in results:\n",
    "                    if success:\n",
    "                        line = json.dumps(example, ensure_ascii=False) + \"\\n\"\n",
    "                        if is_train:\n",
    "                            train_f.write(line)\n",
    "                        else:\n",
    "                            val_f.write(line)\n",
    "                        total_examples += 1\n",
    "                        if has_oracle:\n",
    "                            with_oracle_count += 1\n",
    "                        else:\n",
    "                            without_oracle_count += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "        \n",
    "        finally:\n",
    "            train_f.close()\n",
    "            val_f.close()\n",
    "        \n",
    "        train_count = min(split_idx, total_examples)\n",
    "        val_count = max(0, total_examples - train_count)\n",
    "        \n",
    "        print(f\"\\nâœ“ Created {total_examples} examples ({failed} failed)\")\n",
    "        print(f\"âœ“ Train: {train_count} | Val: {val_count}\")\n",
    "        \n",
    "        return {\n",
    "            \"total\": total_examples,\n",
    "            \"train\": train_count,\n",
    "            \"val\": val_count,\n",
    "            \"failed\": failed,\n",
    "            \"with_oracle\": with_oracle_count,\n",
    "            \"without_oracle\": without_oracle_count\n",
    "        }\n",
    "    \n",
    "    def _process_batch_semantic(self, batch: List, split_idx: int) -> List:\n",
    "        \"\"\"Process batch with semantic distractor search.\"\"\"\n",
    "        # Collect valid items\n",
    "        valid_items = []\n",
    "        for qa, global_idx in batch:\n",
    "            question = qa.get('question', '')\n",
    "            answer = qa.get('answer', '')\n",
    "            chunk_id = qa.get('chunk_id', '')\n",
    "            \n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            \n",
    "            oracle_idx = self.chunk_by_id.get(chunk_id, -1)\n",
    "            if oracle_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            valid_items.append((qa, oracle_idx, global_idx))\n",
    "        \n",
    "        if not valid_items:\n",
    "            return [(None, False, False, False) for _ in batch]\n",
    "        \n",
    "        # Batch find distractors (ONE embedding call for entire batch)\n",
    "        questions = [item[0]['question'] for item in valid_items]\n",
    "        oracle_indices = [item[1] for item in valid_items]\n",
    "        all_distractors = self.find_distractors_batch(questions, oracle_indices, k=self.config.num_distractors + 2)\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        valid_map = {item[2]: i for i, item in enumerate(valid_items)}  # global_idx -> valid_idx\n",
    "        \n",
    "        for qa, global_idx in batch:\n",
    "            if global_idx not in valid_map:\n",
    "                results.append((None, global_idx < split_idx, False, False))\n",
    "                continue\n",
    "            \n",
    "            valid_idx = valid_map[global_idx]\n",
    "            oracle_idx = valid_items[valid_idx][1]\n",
    "            distractor_indices = all_distractors[valid_idx]\n",
    "            \n",
    "            include_oracle = random.random() < self.config.oracle_ratio\n",
    "            \n",
    "            context = []\n",
    "            if include_oracle:\n",
    "                context.append(self.format_context_document(self.chunks[oracle_idx], is_oracle=True))\n",
    "                for idx in distractor_indices[:self.config.num_distractors]:\n",
    "                    context.append(self.format_context_document(self.chunks[idx], is_oracle=False))\n",
    "                final_answer = qa.get('answer', '')\n",
    "                with_oracle_count = True\n",
    "            else:\n",
    "                for idx in distractor_indices[:self.config.num_distractors + 1]:\n",
    "                    context.append(self.format_context_document(self.chunks[idx], is_oracle=False))\n",
    "                final_answer = self._create_no_oracle_answer(qa['question'])\n",
    "                with_oracle_count = False\n",
    "            \n",
    "            random.shuffle(context)\n",
    "            \n",
    "            example = {\n",
    "                \"question\": qa['question'],\n",
    "                \"context\": context,\n",
    "                \"answer\": final_answer,\n",
    "                \"has_oracle\": include_oracle,\n",
    "                \"category\": qa.get('category', 'Unknown'),\n",
    "                \"difficulty\": qa.get('difficulty', 'medium'),\n",
    "                \"question_type\": qa.get('type', 'unknown')\n",
    "            }\n",
    "            \n",
    "            results.append((example, global_idx < split_idx, True, include_oracle))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RAFT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RAFT\n",
    "config = RAFTConfig(\n",
    "    num_distractors=4,      # 4 distractor docs per example\n",
    "    oracle_ratio=0.8,       # 80% have oracle, 20% only distractors\n",
    "    train_ratio=0.9         # 90% train, 10% val\n",
    ")\n",
    "\n",
    "print(\"RAFT Configuration:\")\n",
    "print(f\"  Distractors per example: {config.num_distractors}\")\n",
    "print(f\"  Oracle ratio: {config.oracle_ratio}\")\n",
    "print(f\"  Train/Val split: {config.train_ratio}/{1-config.train_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset with SEMANTIC distractors (proper quality)\n",
    "builder = RAFTDatasetBuilder(\n",
    "    chunks=chunks,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    faiss_index=index,\n",
    "    embed_model=embed_model,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# batch_size=1000 is the sweet spot for speed vs memory\n",
    "stats = builder.build_dataset_streaming(QA_FILE, OUTPUT_DIR, qa_count, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect train set\n",
    "train_path = os.path.join(OUTPUT_DIR, \"train.jsonl\")\n",
    "train_examples = []\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Train examples: {len(train_examples)}\")\n",
    "\n",
    "# Sample example\n",
    "example = train_examples[0]\n",
    "print(\"\\nSample RAFT example:\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Has Oracle: {example['has_oracle']}\")\n",
    "print(f\"Context documents: {len(example['context'])}\")\n",
    "print(f\"Answer preview: {example['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show context structure\n",
    "print(\"\\nContext documents:\")\n",
    "for i, doc in enumerate(example['context']):\n",
    "    print(f\"\\nDoc {i+1}:\")\n",
    "    print(f\"  Source: {doc['source']}\")\n",
    "    print(f\"  Page: {doc['page_number']}\")\n",
    "    print(f\"  Is Oracle: {doc['is_oracle']}\")\n",
    "    print(f\"  Content: {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate no-oracle examples\n",
    "no_oracle = [e for e in train_examples if not e['has_oracle']]\n",
    "print(f\"\\nNo-oracle examples: {len(no_oracle)}\")\n",
    "\n",
    "if no_oracle:\n",
    "    sample = no_oracle[0]\n",
    "    print(f\"\\nSample no-oracle example:\")\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"Answer: {sample['answer']}\")\n",
    "    print(f\"All is_oracle values: {[d['is_oracle'] for d in sample['context']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = os.path.getsize(train_path) / (1024**2)\n",
    "val_path = os.path.join(OUTPUT_DIR, \"val.jsonl\")\n",
    "val_size = os.path.getsize(val_path) / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAFT DATASET CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“Š Total examples: {stats['total']}\")\n",
    "print(f\"ðŸ“ˆ Train examples: {stats['train']} ({train_size:.1f} MB)\")\n",
    "print(f\"ðŸ“‰ Val examples: {stats['val']} ({val_size:.1f} MB)\")\n",
    "print(f\"âœ… With oracle: {stats['with_oracle']} ({100*stats['with_oracle']/stats['total']:.1f}%)\")\n",
    "print(f\"âŒ Without oracle: {stats['without_oracle']} ({100*stats['without_oracle']/stats['total']:.1f}%)\")\n",
    "print(f\"âš ï¸ Failed: {stats['failed']}\")\n",
    "print(f\"ðŸ“ Output: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Run 04_embedding_creation.ipynb (for FAISS index)\")\n",
    "print(\"Then: Run 05_model_training.ipynb (for QLoRA fine-tuning)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
