{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ RAFT Dataset Creation\n",
    "\n",
    "This notebook creates the final RAFT (Retrieval Augmented Fine-Tuning) dataset.\n",
    "\n",
    "**RAFT Format:**\n",
    "- Each example has 1 oracle document (contains answer) + 4 distractor documents\n",
    "- 80% examples: oracle + distractors\n",
    "- 20% examples: only distractors (teach model to say \"I don't know\")\n",
    "\n",
    "**Input:** `qa_pairs.jsonl`, `chunks.jsonl`\n",
    "**Output:** `train.jsonl`, `val.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/content/drive/MyDrive/RAFT_dental_data\"\n",
    "CHUNKS_FILE = f\"{DATA_DIR}/chunks.jsonl\"\n",
    "QA_FILE = f\"{DATA_DIR}/qa_pairs.jsonl\"\n",
    "OUTPUT_DIR = f\"{DATA_DIR}/raft_dataset\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify inputs\n",
    "for f in [CHUNKS_FILE, QA_FILE]:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"âœ“ Found {os.path.basename(f)}\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load chunks\n",
    "chunks = []\n",
    "chunk_by_id = {}\n",
    "with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunk = json.loads(line)\n",
    "        chunks.append(chunk)\n",
    "        chunk_by_id[chunk['chunk_id']] = chunk\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Load Q&A pairs\n",
    "qa_pairs = []\n",
    "with open(QA_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        qa_pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} Q&A pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Embedding Index for Distractor Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embedding model (medical domain)\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "print(f\"âœ“ Model loaded: {embed_model.get_sentence_embedding_dimension()}D embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(\"Generating chunk embeddings...\")\n",
    "\n",
    "chunk_texts = [c['text'] for c in chunks]\n",
    "chunk_embeddings = embed_model.encode(\n",
    "    chunk_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Embeddings shape: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "print(\"Building FAISS index...\")\n",
    "\n",
    "dimension = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product (cosine after normalization)\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(chunk_embeddings)\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "print(f\"âœ“ FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAFT Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RAFTConfig:\n",
    "    \"\"\"Configuration for RAFT dataset creation.\"\"\"\n",
    "    num_distractors: int = 4          # Number of distractor documents\n",
    "    oracle_ratio: float = 0.8         # Ratio of examples with oracle\n",
    "    max_context_length: int = 3000    # Max tokens for context\n",
    "    train_ratio: float = 0.9          # Train/val split\n",
    "\n",
    "\n",
    "class RAFTDatasetBuilder:\n",
    "    \"\"\"Build RAFT dataset from Q&A pairs and chunks.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        chunk_embeddings: np.ndarray,\n",
    "        faiss_index: faiss.Index,\n",
    "        embed_model: SentenceTransformer,\n",
    "        config: RAFTConfig = None\n",
    "    ):\n",
    "        self.chunks = chunks\n",
    "        self.chunk_embeddings = chunk_embeddings\n",
    "        self.index = faiss_index\n",
    "        self.embed_model = embed_model\n",
    "        self.config = config or RAFTConfig()\n",
    "        \n",
    "        # Create chunk lookup\n",
    "        self.chunk_by_id = {c['chunk_id']: i for i, c in enumerate(chunks)}\n",
    "    \n",
    "    def find_distractors(\n",
    "        self,\n",
    "        question: str,\n",
    "        oracle_idx: int,\n",
    "        k: int = 10\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Find distractor documents similar to question but not the oracle.\"\"\"\n",
    "        # Embed question\n",
    "        q_embedding = self.embed_model.encode([question], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_embedding)\n",
    "        \n",
    "        # Search\n",
    "        _, indices = self.index.search(q_embedding, k + 5)\n",
    "        \n",
    "        # Filter out oracle and return top k\n",
    "        distractors = [idx for idx in indices[0] if idx != oracle_idx]\n",
    "        return distractors[:k]\n",
    "    \n",
    "    def format_context_document(self, chunk: Dict, is_oracle: bool = False) -> Dict:\n",
    "        \"\"\"Format a chunk as a context document.\"\"\"\n",
    "        return {\n",
    "            \"content\": chunk['text'],\n",
    "            \"source\": chunk.get('source', 'Unknown'),\n",
    "            \"page_number\": chunk.get('page_number', 0),\n",
    "            \"category\": chunk.get('category', 'Unknown'),\n",
    "            \"is_oracle\": is_oracle\n",
    "        }\n",
    "    \n",
    "    def create_raft_example(\n",
    "        self,\n",
    "        qa: Dict,\n",
    "        include_oracle: bool = True\n",
    "    ) -> Optional[Dict]:\n",
    "        \"\"\"Create a single RAFT example.\"\"\"\n",
    "        question = qa.get('question', '')\n",
    "        answer = qa.get('answer', '')\n",
    "        chunk_id = qa.get('chunk_id', '')\n",
    "        \n",
    "        if not question or not answer:\n",
    "            return None\n",
    "        \n",
    "        # Get oracle chunk index\n",
    "        oracle_idx = self.chunk_by_id.get(chunk_id, -1)\n",
    "        if oracle_idx == -1:\n",
    "            # Try to find by text match\n",
    "            context_text = qa.get('context_text', '')\n",
    "            for i, c in enumerate(self.chunks):\n",
    "                if c['text'] == context_text:\n",
    "                    oracle_idx = i\n",
    "                    break\n",
    "        \n",
    "        if oracle_idx == -1:\n",
    "            return None\n",
    "        \n",
    "        # Get distractors\n",
    "        distractor_indices = self.find_distractors(\n",
    "            question,\n",
    "            oracle_idx,\n",
    "            k=self.config.num_distractors\n",
    "        )\n",
    "        \n",
    "        # Build context\n",
    "        context = []\n",
    "        \n",
    "        if include_oracle:\n",
    "            # Add oracle document\n",
    "            oracle_chunk = self.chunks[oracle_idx]\n",
    "            context.append(self.format_context_document(oracle_chunk, is_oracle=True))\n",
    "            \n",
    "            # Add distractors\n",
    "            for idx in distractor_indices[:self.config.num_distractors]:\n",
    "                context.append(self.format_context_document(self.chunks[idx], is_oracle=False))\n",
    "        else:\n",
    "            # Only distractors (no oracle)\n",
    "            for idx in distractor_indices[:self.config.num_distractors + 1]:\n",
    "                context.append(self.format_context_document(self.chunks[idx], is_oracle=False))\n",
    "            \n",
    "            # Modify answer for no-oracle case\n",
    "            answer = self._create_no_oracle_answer(question)\n",
    "        \n",
    "        # Shuffle context so oracle isn't always first\n",
    "        random.shuffle(context)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"answer\": answer,\n",
    "            \"has_oracle\": include_oracle,\n",
    "            \"category\": qa.get('category', 'Unknown'),\n",
    "            \"difficulty\": qa.get('difficulty', 'medium'),\n",
    "            \"question_type\": qa.get('type', 'unknown')\n",
    "        }\n",
    "    \n",
    "    def _create_no_oracle_answer(self, question: str) -> str:\n",
    "        \"\"\"Create answer for when oracle is not present.\"\"\"\n",
    "        return (\n",
    "            \"Based on the provided documents, I cannot find sufficient information \"\n",
    "            \"to accurately answer this question. The documents do not contain \"\n",
    "            \"relevant content about this specific topic. Please provide additional \"\n",
    "            \"context or consult a more specialized dental reference.\"\n",
    "        )\n",
    "    \n",
    "    def build_dataset(\n",
    "        self,\n",
    "        qa_pairs: List[Dict],\n",
    "        output_dir: str\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"Build complete RAFT dataset.\"\"\"\n",
    "        print(f\"Building RAFT dataset from {len(qa_pairs)} Q&A pairs...\")\n",
    "        \n",
    "        raft_examples = []\n",
    "        failed = 0\n",
    "        \n",
    "        for qa in tqdm(qa_pairs, desc=\"Creating examples\"):\n",
    "            # Decide if this example gets oracle\n",
    "            include_oracle = random.random() < self.config.oracle_ratio\n",
    "            \n",
    "            example = self.create_raft_example(qa, include_oracle)\n",
    "            \n",
    "            if example:\n",
    "                raft_examples.append(example)\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        print(f\"Created {len(raft_examples)} examples ({failed} failed)\")\n",
    "        \n",
    "        # Split train/val\n",
    "        random.shuffle(raft_examples)\n",
    "        split_idx = int(len(raft_examples) * self.config.train_ratio)\n",
    "        \n",
    "        train_examples = raft_examples[:split_idx]\n",
    "        val_examples = raft_examples[split_idx:]\n",
    "        \n",
    "        # Save\n",
    "        train_path = os.path.join(output_dir, \"train.jsonl\")\n",
    "        val_path = os.path.join(output_dir, \"val.jsonl\")\n",
    "        \n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            for ex in train_examples:\n",
    "                f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        with open(val_path, 'w', encoding='utf-8') as f:\n",
    "            for ex in val_examples:\n",
    "                f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved {len(train_examples)} train examples to {train_path}\")\n",
    "        print(f\"âœ“ Saved {len(val_examples)} val examples to {val_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"total\": len(raft_examples),\n",
    "            \"train\": len(train_examples),\n",
    "            \"val\": len(val_examples),\n",
    "            \"failed\": failed,\n",
    "            \"with_oracle\": sum(1 for e in raft_examples if e['has_oracle']),\n",
    "            \"without_oracle\": sum(1 for e in raft_examples if not e['has_oracle'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RAFT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RAFT\n",
    "config = RAFTConfig(\n",
    "    num_distractors=4,      # 4 distractor docs per example\n",
    "    oracle_ratio=0.8,       # 80% have oracle, 20% only distractors\n",
    "    train_ratio=0.9         # 90% train, 10% val\n",
    ")\n",
    "\n",
    "print(\"RAFT Configuration:\")\n",
    "print(f\"  Distractors per example: {config.num_distractors}\")\n",
    "print(f\"  Oracle ratio: {config.oracle_ratio}\")\n",
    "print(f\"  Train/Val split: {config.train_ratio}/{1-config.train_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "builder = RAFTDatasetBuilder(\n",
    "    chunks=chunks,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    faiss_index=index,\n",
    "    embed_model=embed_model,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "stats = builder.build_dataset(qa_pairs, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect train set\n",
    "train_path = os.path.join(OUTPUT_DIR, \"train.jsonl\")\n",
    "train_examples = []\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Train examples: {len(train_examples)}\")\n",
    "\n",
    "# Sample example\n",
    "example = train_examples[0]\n",
    "print(\"\\nSample RAFT example:\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Has Oracle: {example['has_oracle']}\")\n",
    "print(f\"Context documents: {len(example['context'])}\")\n",
    "print(f\"Answer preview: {example['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show context structure\n",
    "print(\"\\nContext documents:\")\n",
    "for i, doc in enumerate(example['context']):\n",
    "    print(f\"\\nDoc {i+1}:\")\n",
    "    print(f\"  Source: {doc['source']}\")\n",
    "    print(f\"  Page: {doc['page_number']}\")\n",
    "    print(f\"  Is Oracle: {doc['is_oracle']}\")\n",
    "    print(f\"  Content: {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate no-oracle examples\n",
    "no_oracle = [e for e in train_examples if not e['has_oracle']]\n",
    "print(f\"\\nNo-oracle examples: {len(no_oracle)}\")\n",
    "\n",
    "if no_oracle:\n",
    "    sample = no_oracle[0]\n",
    "    print(f\"\\nSample no-oracle example:\")\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"Answer: {sample['answer']}\")\n",
    "    print(f\"All is_oracle values: {[d['is_oracle'] for d in sample['context']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = os.path.getsize(train_path) / (1024**2)\n",
    "val_path = os.path.join(OUTPUT_DIR, \"val.jsonl\")\n",
    "val_size = os.path.getsize(val_path) / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAFT DATASET CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“Š Total examples: {stats['total']}\")\n",
    "print(f\"ðŸ“ˆ Train examples: {stats['train']} ({train_size:.1f} MB)\")\n",
    "print(f\"ðŸ“‰ Val examples: {stats['val']} ({val_size:.1f} MB)\")\n",
    "print(f\"âœ… With oracle: {stats['with_oracle']} ({100*stats['with_oracle']/stats['total']:.1f}%)\")\n",
    "print(f\"âŒ Without oracle: {stats['without_oracle']} ({100*stats['without_oracle']/stats['total']:.1f}%)\")\n",
    "print(f\"âš ï¸ Failed: {stats['failed']}\")\n",
    "print(f\"ðŸ“ Output: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Run 04_embedding_creation.ipynb (for FAISS index)\")\n",
    "print(\"Then: Run 05_model_training.ipynb (for QLoRA fine-tuning)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
