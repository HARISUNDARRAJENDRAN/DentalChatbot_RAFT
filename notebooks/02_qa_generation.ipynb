{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶∑ Q&A Generation for RAFT Dataset\n",
    "\n",
    "This notebook generates question-answer pairs from dental text chunks.\n",
    "\n",
    "**Input:** `chunks.jsonl` from notebook 01\n",
    "**Output:** `qa_pairs.jsonl` for RAFT formatting\n",
    "\n",
    "**API:** Uses Groq (FREE) or OpenAI for generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q groq openai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/content/drive/MyDrive/RAFT_dental_data\"\n",
    "CHUNKS_FILE = f\"{DATA_DIR}/chunks.jsonl\"\n",
    "OUTPUT_FILE = f\"{DATA_DIR}/qa_pairs.jsonl\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(CHUNKS_FILE):\n",
    "    print(f\"‚úì Found chunks file\")\n",
    "else:\n",
    "    print(f\"‚úó chunks.jsonl not found. Run notebook 01 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - Choose ONE:\n",
    "# Option 1: Groq (FREE - recommended)\n",
    "# Get your key at: https://console.groq.com/keys\n",
    "GROQ_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Option 2: OpenAI (paid but higher quality)\n",
    "OPENAI_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Select provider\n",
    "USE_GROQ = True  # Set False to use OpenAI\n",
    "\n",
    "if USE_GROQ and GROQ_API_KEY:\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úì Using Groq API (FREE)\")\n",
    "elif OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"‚úì Using OpenAI API\")\n",
    "else:\n",
    "    print(\"‚ö† No API key provided. Enter your key above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load all chunks\n",
    "chunks = []\n",
    "with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Sample chunk\n",
    "print(\"\\nSample chunk:\")\n",
    "print(f\"Source: {chunks[0]['source']}\")\n",
    "print(f\"Category: {chunks[0]['category']}\")\n",
    "print(f\"Text preview: {chunks[0]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q&A Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Prompts\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert dental educator creating study materials for dental students.\n",
    "Generate high-quality question-answer pairs from the provided dental textbook content.\n",
    "\n",
    "Requirements:\n",
    "1. Generate 3-5 questions per passage\n",
    "2. Questions should test understanding, not just recall\n",
    "3. Include a mix of: conceptual, clinical application, and comparison questions\n",
    "4. Answers MUST include direct quotes from the text using ##begin_quote## and ##end_quote## markers\n",
    "5. Answers should explain the concept, not just quote\n",
    "\n",
    "Output format (JSON array):\n",
    "[\n",
    "  {\n",
    "    \"question\": \"What are the indications for...\",\n",
    "    \"answer\": \"According to the text, ##begin_quote##exact quote here##end_quote##, this means that...\",\n",
    "    \"difficulty\": \"medium\",\n",
    "    \"type\": \"conceptual\"\n",
    "  }\n",
    "]\n",
    "\n",
    "Question types: conceptual, clinical, procedural, comparison, definition\n",
    "Difficulty levels: easy, medium, hard\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Generate Q&A pairs from this dental textbook passage:\n",
    "\n",
    "Source: {source}\n",
    "Category: {category}\n",
    "Page: {page_number}\n",
    "\n",
    "Content:\n",
    "{text}\n",
    "\n",
    "Generate 3-5 question-answer pairs. Return ONLY valid JSON array.\"\"\"\n",
    "\n",
    "\n",
    "class QAGenerator:\n",
    "    \"\"\"Generate Q&A pairs using LLM APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_groq: bool = True):\n",
    "        self.use_groq = use_groq\n",
    "        \n",
    "        if use_groq:\n",
    "            from groq import Groq\n",
    "            self.client = Groq()\n",
    "            self.model = \"llama-3.1-70b-versatile\"\n",
    "        else:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.model = \"gpt-4-turbo-preview\"\n",
    "        \n",
    "        self.rate_limit_delay = 1.0 if use_groq else 0.5\n",
    "    \n",
    "    def generate(self, chunk: Dict) -> List[Dict]:\n",
    "        \"\"\"Generate Q&A pairs from a chunk.\"\"\"\n",
    "        user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "            source=chunk.get('source', 'Unknown'),\n",
    "            category=chunk.get('category', 'Unknown'),\n",
    "            page_number=chunk.get('page_number', 0),\n",
    "            text=chunk['text']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            if self.use_groq:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "            else:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            qa_pairs = self._parse_response(content)\n",
    "            \n",
    "            # Add metadata\n",
    "            for qa in qa_pairs:\n",
    "                qa['chunk_id'] = chunk.get('chunk_id', '')\n",
    "                qa['source'] = chunk.get('source', '')\n",
    "                qa['category'] = chunk.get('category', '')\n",
    "                qa['page_number'] = chunk.get('page_number', 0)\n",
    "                qa['context_text'] = chunk['text']\n",
    "            \n",
    "            time.sleep(self.rate_limit_delay)\n",
    "            return qa_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Q&A: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_response(self, content: str) -> List[Dict]:\n",
    "        \"\"\"Parse LLM response to extract Q&A pairs.\"\"\"\n",
    "        try:\n",
    "            # Try direct JSON parse\n",
    "            data = json.loads(content)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            elif isinstance(data, dict) and 'questions' in data:\n",
    "                return data['questions']\n",
    "            elif isinstance(data, dict) and 'qa_pairs' in data:\n",
    "                return data['qa_pairs']\n",
    "            return []\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract JSON from markdown code blocks\n",
    "            match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', content)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group(1))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Try to find JSON array\n",
    "            match = re.search(r'\\[\\s*\\{[\\s\\S]*\\}\\s*\\]', content)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group(0))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return []\n",
    "\n",
    "\n",
    "# Initialize generator\n",
    "generator = QAGenerator(use_groq=USE_GROQ)\n",
    "print(f\"‚úì Initialized {generator.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one chunk\n",
    "test_chunk = chunks[0]\n",
    "print(f\"Testing with: {test_chunk['source']}\")\n",
    "print(f\"Category: {test_chunk['category']}\")\n",
    "print()\n",
    "\n",
    "test_qa = generator.generate(test_chunk)\n",
    "print(f\"Generated {len(test_qa)} Q&A pairs:\\n\")\n",
    "\n",
    "for i, qa in enumerate(test_qa, 1):\n",
    "    print(f\"Q{i}: {qa.get('question', 'N/A')}\")\n",
    "    print(f\"A{i}: {qa.get('answer', 'N/A')[:200]}...\")\n",
    "    print(f\"Type: {qa.get('type', 'N/A')}, Difficulty: {qa.get('difficulty', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate All Q&A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_CHUNKS = None  # Set to number to limit, None for all\n",
    "SKIP_EXISTING = True  # Skip if output file exists\n",
    "CHECKPOINT_EVERY = 100  # Save progress every N chunks\n",
    "\n",
    "# Check for existing progress\n",
    "existing_qa = []\n",
    "processed_chunk_ids = set()\n",
    "\n",
    "if SKIP_EXISTING and os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            qa = json.loads(line)\n",
    "            existing_qa.append(qa)\n",
    "            processed_chunk_ids.add(qa.get('chunk_id', ''))\n",
    "    print(f\"Found {len(existing_qa)} existing Q&A pairs\")\n",
    "    print(f\"Skipping {len(processed_chunk_ids)} already processed chunks\")\n",
    "\n",
    "# Filter chunks to process\n",
    "chunks_to_process = [c for c in chunks if c.get('chunk_id', '') not in processed_chunk_ids]\n",
    "if MAX_CHUNKS:\n",
    "    chunks_to_process = chunks_to_process[:MAX_CHUNKS]\n",
    "\n",
    "print(f\"\\nChunks to process: {len(chunks_to_process)}\")\n",
    "print(f\"Estimated Q&A pairs: {len(chunks_to_process) * 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Q&A pairs\n",
    "all_qa = existing_qa.copy()\n",
    "failed = 0\n",
    "\n",
    "with open(OUTPUT_FILE, 'a' if existing_qa else 'w', encoding='utf-8') as f:\n",
    "    for i, chunk in enumerate(tqdm(chunks_to_process, desc=\"Generating Q&A\")):\n",
    "        try:\n",
    "            qa_pairs = generator.generate(chunk)\n",
    "            \n",
    "            for qa in qa_pairs:\n",
    "                f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n",
    "                all_qa.append(qa)\n",
    "            \n",
    "            # Checkpoint\n",
    "            if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "                f.flush()\n",
    "                tqdm.write(f\"Checkpoint: {len(all_qa)} Q&A pairs saved\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            tqdm.write(f\"Failed on chunk {chunk.get('chunk_id', i)}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(all_qa)} total Q&A pairs\")\n",
    "print(f\"Failed chunks: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final Q&A pairs\n",
    "qa_pairs = []\n",
    "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        qa_pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Total Q&A pairs: {len(qa_pairs)}\")\n",
    "\n",
    "# Distribution by category\n",
    "print(\"\\nBy Category:\")\n",
    "category_counts = {}\n",
    "for qa in qa_pairs:\n",
    "    cat = qa.get('category', 'Unknown')\n",
    "    category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "\n",
    "for cat, count in sorted(category_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Distribution by type\n",
    "print(\"\\nBy Question Type:\")\n",
    "type_counts = {}\n",
    "for qa in qa_pairs:\n",
    "    qtype = qa.get('type', 'unknown')\n",
    "    type_counts[qtype] = type_counts.get(qtype, 0) + 1\n",
    "\n",
    "for qtype, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {qtype}: {count}\")\n",
    "\n",
    "# Check citation markers\n",
    "with_citations = sum(1 for qa in qa_pairs if '##begin_quote##' in qa.get('answer', ''))\n",
    "print(f\"\\nAnswers with citations: {with_citations}/{len(qa_pairs)} ({100*with_citations/len(qa_pairs):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Q&A pairs\n",
    "import random\n",
    "\n",
    "print(\"Sample Q&A pairs:\\n\")\n",
    "for qa in random.sample(qa_pairs, min(3, len(qa_pairs))):\n",
    "    print(f\"Category: {qa.get('category', 'N/A')}\")\n",
    "    print(f\"Q: {qa.get('question', 'N/A')}\")\n",
    "    print(f\"A: {qa.get('answer', 'N/A')[:300]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = os.path.getsize(OUTPUT_FILE) / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q&A GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìù Total Q&A pairs: {len(qa_pairs)}\")\n",
    "print(f\"üìÅ Output file: {OUTPUT_FILE}\")\n",
    "print(f\"üíæ File size: {file_size:.1f} MB\")\n",
    "print(f\"üìä Categories covered: {len(category_counts)}\")\n",
    "print(f\"‚úÖ Citation rate: {100*with_citations/len(qa_pairs):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Run 03_raft_dataset_creation.ipynb\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
