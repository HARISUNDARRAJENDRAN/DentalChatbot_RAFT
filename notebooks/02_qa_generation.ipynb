{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦· Q&A Generation for RAFT Dataset (ULTRA FAST + HIGH QUALITY)\n",
    "\n",
    "This notebook generates high-quality question-answer pairs from dental text chunks using **Google Gemini 2.0 Flash**.\n",
    "\n",
    "**Features:**\n",
    "- âš¡ Ultra fast parallel processing (20 concurrent requests)\n",
    "- ğŸ¯ Quality filters to skip junk chunks\n",
    "- ğŸ“š 8-10 clinical/educational questions per chunk\n",
    "- ğŸ’¾ Auto-resume from checkpoint\n",
    "\n",
    "**Input:** `chunks.jsonl` from notebook 01\n",
    "**Output:** `qa_pairs.jsonl` for RAFT formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q google-generativeai tqdm nest_asyncio aiohttp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/content/drive/MyDrive/RAFT_dental_data\"\n",
    "CHUNKS_FILE = f\"{DATA_DIR}/chunks.jsonl\"\n",
    "OUTPUT_FILE = f\"{DATA_DIR}/qa_pairs.jsonl\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(CHUNKS_FILE):\n",
    "    print(f\"âœ“ Found chunks file\")\n",
    "else:\n",
    "    print(f\"âœ— chunks.jsonl not found. Run notebook 01 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# ğŸ‘‡ Paste your GCP Gemini API key here\n",
    "GEMINI_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# âš¡ ULTRA FAST SETTINGS\n",
    "MAX_CONCURRENT = 20         # 20 parallel requests\n",
    "DELAY_BETWEEN_BATCHES = 0.1 # Minimal delay\n",
    "BATCH_SIZE = 20             # 20 chunks per batch\n",
    "\n",
    "# Processing settings\n",
    "TIMEOUT_HOURS = 12\n",
    "CHECKPOINT_EVERY = 200\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "print(\"âš¡ ULTRA FAST Configuration:\")\n",
    "print(f\"  Concurrent requests: {MAX_CONCURRENT}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Questions per chunk: 8-10\")\n",
    "print(f\"  Timeout: {TIMEOUT_HOURS} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Gemini 2.0 Flash (FAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import google.generativeai as genai\nimport json\nimport time\nimport re\nimport random\nimport asyncio\nimport aiohttp\nfrom datetime import datetime, timedelta\nfrom tqdm.notebook import tqdm\nimport threading\n\n# Configure Gemini\ngenai.configure(api_key=GEMINI_API_KEY)\n\n# âš¡ Use FAST model with async\nmodel = genai.GenerativeModel(\n    model_name='gemini-2.0-flash',\n    generation_config={\n        'temperature': 0.7,\n        'top_p': 0.9,\n        'max_output_tokens': 3000,\n    }\n)\n\n# Test connection\nprint(\"Testing Gemini 2.0 Flash connection...\")\ntry:\n    test_response = model.generate_content(\"Say 'OK' only\")\n    print(f\"âœ“ Connected! Response: {test_response.text.strip()}\")\nexcept Exception as e:\n    print(f\"âœ— Connection failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all chunks\n",
    "chunks = []\n",
    "with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "print(f\"âœ“ Loaded {len(chunks)} total chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Filter + Fast Q&A Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUALITY FILTER =====\n",
    "\n",
    "def is_valid_chunk(chunk: dict) -> bool:\n",
    "    \"\"\"Check if chunk has enough educational dental content.\"\"\"\n",
    "    text = chunk.get('text', '').lower()\n",
    "    \n",
    "    # Skip title pages, ToC, index, etc.\n",
    "    skip_patterns = [\n",
    "        'table of contents', 'contents', 'copyright', 'all rights reserved',\n",
    "        'isbn', 'printed in', 'library of congress', 'publisher',\n",
    "        'preface', 'acknowledgment', 'dedication', 'about the author',\n",
    "        'index', 'bibliography', 'further reading', 'contributors',\n",
    "        'first published', 'reprinted', 'edited by'\n",
    "    ]\n",
    "    \n",
    "    for pattern in skip_patterns:\n",
    "        if pattern in text[:500]:\n",
    "            return False\n",
    "    \n",
    "    # Must have minimum content\n",
    "    if len(text) < 300:\n",
    "        return False\n",
    "    \n",
    "    # Should have dental/medical terms\n",
    "    dental_terms = [\n",
    "        'tooth', 'teeth', 'dental', 'oral', 'pulp', 'enamel', 'dentin',\n",
    "        'gingiva', 'periodon', 'endo', 'crown', 'root', 'cavity', 'caries',\n",
    "        'restoration', 'implant', 'occlus', 'mandib', 'maxill', 'bone',\n",
    "        'tissue', 'nerve', 'treatment', 'patient', 'clinical', 'diagnosis',\n",
    "        'procedure', 'symptom', 'disease', 'infect', 'inflam', 'pain',\n",
    "        'surgery', 'anesthes', 'extract', 'fill', 'canal', 'apex',\n",
    "        'mucosa', 'saliva', 'jaw', 'bite', 'chew', 'molar', 'incisor',\n",
    "        'premolar', 'canine', 'amalgam', 'composite', 'cement', 'x-ray',\n",
    "        'radiograph', 'bacteria', 'plaque', 'calculus', 'bleeding'\n",
    "    ]\n",
    "    \n",
    "    term_count = sum(1 for term in dental_terms if term in text)\n",
    "    return term_count >= 3\n",
    "\n",
    "\n",
    "# Pre-filter chunks\n",
    "valid_chunks = [c for c in chunks if is_valid_chunk(c)]\n",
    "print(f\"âœ“ Valid chunks: {len(valid_chunks)} / {len(chunks)} ({100*len(valid_chunks)//len(chunks)}%)\")\n",
    "print(f\"  Filtered out {len(chunks) - len(valid_chunks)} low-quality chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== DIRECT API CALLS WITH AIOHTTP (GUARANTEED PARALLEL) =====\n\nimport aiohttp\n\nSYSTEM_PROMPT = \"\"\"You are a dental educator creating exam questions. Generate 8-10 high-quality questions from the text.\n\nRULES:\n- Generate exactly 8-10 questions\n- ONLY clinical/educational dental content\n- NO questions about book titles, authors, editions, page numbers\n- Answers must cite text using ##begin_quote##...##end_quote##\n- Mix of question types: clinical scenarios, conceptual, procedural, definitions\n- Mix of difficulty: easy, medium, hard\n- Return [] if no educational content\n\nReturn JSON only:\n[{\"question\":\"...\",\"answer\":\"Based on the text, ##begin_quote##...##end_quote##. This means...\",\"type\":\"clinical\",\"difficulty\":\"medium\"}]\"\"\"\n\nAPI_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}\"\n\n\nasync def generate_qa_direct(session: aiohttp.ClientSession, chunk: dict, semaphore: asyncio.Semaphore) -> list:\n    \"\"\"Generate Q&A using direct API call - TRULY PARALLEL.\"\"\"\n    async with semaphore:\n        prompt = f\"\"\"{SYSTEM_PROMPT}\n\nText ({chunk.get('category', 'Dental')}):\n{chunk['text'][:3000]}\n\nGenerate 8-10 questions. JSON:\"\"\"\n\n        payload = {\n            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n            \"generationConfig\": {\n                \"temperature\": 0.7,\n                \"topP\": 0.9,\n                \"maxOutputTokens\": 3000\n            }\n        }\n\n        for attempt in range(MAX_RETRIES):\n            try:\n                async with session.post(API_URL, json=payload, timeout=aiohttp.ClientTimeout(total=60)) as resp:\n                    if resp.status == 429:\n                        await asyncio.sleep(2)\n                        continue\n                    \n                    data = await resp.json()\n                    \n                    if 'candidates' not in data:\n                        return []\n                    \n                    text = data['candidates'][0]['content']['parts'][0]['text'].strip()\n\n                    # Clean markdown\n                    text = re.sub(r'^```json\\s*', '', text)\n                    text = re.sub(r'^```\\s*', '', text)\n                    text = re.sub(r'\\s*```$', '', text)\n\n                    qa_pairs = json.loads(text)\n\n                    # Filter and add metadata\n                    valid = []\n                    for qa in qa_pairs:\n                        q = qa.get('question', '').lower()\n                        if any(s in q for s in ['title', 'author', 'edition', 'publisher', 'copyright', 'page']):\n                            continue\n                        if 'question' in qa and 'answer' in qa and len(qa['answer']) > 30:\n                            qa['chunk_id'] = chunk.get('chunk_id', '')\n                            qa['source'] = chunk.get('source', '')\n                            qa['category'] = chunk.get('category', '')\n                            qa['page_number'] = chunk.get('page_number', 0)\n                            qa['context_text'] = chunk['text']\n                            valid.append(qa)\n\n                    return valid\n\n            except Exception as e:\n                if attempt < MAX_RETRIES - 1:\n                    await asyncio.sleep(1)\n                continue\n\n        return []\n\n\nprint(\"âœ“ Direct API Q&A generator ready (GUARANTEED parallel with aiohttp)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with a good chunk using direct API\nimport nest_asyncio\nnest_asyncio.apply()\n\ntest_chunk = valid_chunks[100]\nprint(f\"Testing: {test_chunk['source'][:60]}...\")\nprint(f\"Category: {test_chunk['category']}\")\nprint(f\"Text preview: {test_chunk['text'][:200]}...\\n\")\n\nasync def test_generation():\n    semaphore = asyncio.Semaphore(1)\n    connector = aiohttp.TCPConnector(limit=1)\n    async with aiohttp.ClientSession(connector=connector) as session:\n        return await generate_qa_direct(session, test_chunk, semaphore)\n\ntest_qa = asyncio.run(test_generation())\nprint(f\"Generated {len(test_qa)} Q&A pairs:\\n\")\n\nfor i, qa in enumerate(test_qa[:5], 1):\n    print(f\"Q{i}: {qa['question']}\")\n    print(f\"A{i}: {qa['answer'][:120]}...\")\n    print(f\"Type: {qa.get('type')}, Difficulty: {qa.get('difficulty')}\\n\")\n\nif len(test_qa) > 5:\n    print(f\"... and {len(test_qa) - 5} more questions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Existing Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing Q&A pairs (for resume)\n",
    "existing_qa = []\n",
    "processed_chunk_ids = set()\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                qa = json.loads(line)\n",
    "                existing_qa.append(qa)\n",
    "                processed_chunk_ids.add(qa.get('chunk_id', ''))\n",
    "            except:\n",
    "                continue\n",
    "    print(f\"âœ“ Found {len(existing_qa)} existing Q&A pairs\")\n",
    "    print(f\"âœ“ {len(processed_chunk_ids)} chunks already processed\")\n",
    "else:\n",
    "    print(\"No existing progress. Starting fresh.\")\n",
    "\n",
    "# Filter unprocessed chunks\n",
    "chunks_to_process = [c for c in valid_chunks if c.get('chunk_id', '') not in processed_chunk_ids]\n",
    "print(f\"\\nğŸ“‹ Chunks to process: {len(chunks_to_process)}\")\n",
    "print(f\"ğŸ“ Expected Q&A pairs: ~{len(chunks_to_process) * 9} (9 per chunk)\")\n",
    "\n",
    "# Estimate time (with fast settings)\n",
    "est_time_min = len(chunks_to_process) / (MAX_CONCURRENT * 3) / 60  # ~3 per second per worker\n",
    "print(f\"â±ï¸ Estimated time: ~{est_time_min:.0f} minutes (~{est_time_min/60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate All Q&A Pairs (ULTRA FAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== TRUE PARALLEL GENERATION WITH AIOHTTP =====\n\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Increase concurrency - Gemini allows high rate\nMAX_CONCURRENT = 50  # 50 parallel requests!\n\nSTART_TIME = time.time()\nEND_TIME = START_TIME + (TIMEOUT_HOURS * 3600)\n\nprint(\"=\"*60)\nprint(\"ğŸš€ STARTING TRUE PARALLEL Q&A GENERATION (aiohttp)\")\nprint(\"=\"*60)\nprint(f\"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"âš¡ Concurrent requests: {MAX_CONCURRENT}\")\nprint(f\"ğŸ“Š Chunks to process: {len(chunks_to_process)}\")\nprint(f\"â“ Questions per chunk: 8-10\")\nprint(f\"ğŸ’¾ Output: {OUTPUT_FILE}\")\nprint(\"=\"*60)\nprint()\n\nall_qa = existing_qa.copy()\ntotal_new = 0\nfailed_chunks = 0\n\nasync def process_all_chunks_fast(chunks_list):\n    \"\"\"Process all chunks with GUARANTEED parallel aiohttp calls.\"\"\"\n    global total_new, failed_chunks\n    \n    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, limit_per_host=MAX_CONCURRENT)\n    \n    async with aiohttp.ClientSession(connector=connector) as session:\n        \n        # Process in batches of 1000\n        BATCH_SIZE = 1000\n        \n        with open(OUTPUT_FILE, 'a' if existing_qa else 'w', encoding='utf-8') as f:\n            for batch_start in range(0, len(chunks_list), BATCH_SIZE):\n                batch_end = min(batch_start + BATCH_SIZE, len(chunks_list))\n                batch = chunks_list[batch_start:batch_end]\n                \n                print(f\"\\nğŸ“¦ Batch {batch_start//BATCH_SIZE + 1}/{(len(chunks_list)-1)//BATCH_SIZE + 1} ({len(batch)} chunks)\")\n                \n                # Create all tasks\n                tasks = [generate_qa_direct(session, chunk, semaphore) for chunk in batch]\n                \n                # Run with progress bar\n                batch_qa = 0\n                batch_failed = 0\n                \n                with tqdm(total=len(tasks), desc=\"Generating\") as pbar:\n                    for coro in asyncio.as_completed(tasks):\n                        try:\n                            result = await coro\n                            if result:\n                                for qa in result:\n                                    f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n                                    all_qa.append(qa)\n                                    total_new += 1\n                                    batch_qa += 1\n                            else:\n                                batch_failed += 1\n                                failed_chunks += 1\n                        except Exception as e:\n                            batch_failed += 1\n                            failed_chunks += 1\n                        pbar.update(1)\n                \n                f.flush()\n                elapsed = (time.time() - START_TIME) / 60\n                rate = total_new / elapsed if elapsed > 0 else 0\n                chunks_done = batch_start + len(batch)\n                chunks_rate = chunks_done / elapsed if elapsed > 0 else 0\n                eta = (len(chunks_list) - chunks_done) / chunks_rate if chunks_rate > 0 else 0\n                \n                print(f\"âœ… +{batch_qa} Q&A (failed: {batch_failed}) | Total: {total_new} | {rate:.0f} Q&A/min | ETA: {eta:.0f} min\")\n                \n                if time.time() > END_TIME:\n                    print(\"â° Timeout!\")\n                    break\n\n# Run\nasyncio.run(process_all_chunks_fast(chunks_to_process))\n\n# Final stats\nelapsed_total = (time.time() - START_TIME) / 60\n\nprint()\nprint(\"=\"*60)\nprint(\"âœ… GENERATION COMPLETE\")\nprint(\"=\"*60)\nprint(f\"ğŸ“ Total Q&A pairs: {len(all_qa)}\")\nprint(f\"âœ… New Q&A generated: {total_new}\")\nprint(f\"âŒ Failed chunks: {failed_chunks}\")\nprint(f\"â±ï¸ Time: {elapsed_total:.1f} min ({elapsed_total/60:.2f} hrs)\")\nif elapsed_total > 0:\n    print(f\"âš¡ Speed: {total_new/elapsed_total:.0f} Q&A/min | {len(all_qa)/(elapsed_total)*9:.0f} chunks/min\")\nprint(f\"ğŸ“ Saved to: {OUTPUT_FILE}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze Q&A pairs\n",
    "qa_pairs = []\n",
    "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"Total Q&A pairs: {len(qa_pairs)}\")\n",
    "\n",
    "# By category\n",
    "print(\"\\nğŸ“Š By Category:\")\n",
    "cat_counts = {}\n",
    "for qa in qa_pairs:\n",
    "    cat = qa.get('category', 'Unknown')\n",
    "    cat_counts[cat] = cat_counts.get(cat, 0) + 1\n",
    "for cat, count in sorted(cat_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# By type\n",
    "print(\"\\nğŸ“Š By Question Type:\")\n",
    "type_counts = {}\n",
    "for qa in qa_pairs:\n",
    "    t = qa.get('type', 'unknown')\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# By difficulty\n",
    "print(\"\\nğŸ“Š By Difficulty:\")\n",
    "diff_counts = {}\n",
    "for qa in qa_pairs:\n",
    "    d = qa.get('difficulty', 'unknown')\n",
    "    diff_counts[d] = diff_counts.get(d, 0) + 1\n",
    "for d, count in sorted(diff_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {d}: {count}\")\n",
    "\n",
    "# Citation rate\n",
    "with_citations = sum(1 for qa in qa_pairs if '##begin_quote##' in qa.get('answer', ''))\n",
    "print(f\"\\nâœ… Citation rate: {100*with_citations/len(qa_pairs):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Q&A pairs\n",
    "import random\n",
    "\n",
    "print(\"ğŸ“ Sample Q&A pairs:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for qa in random.sample(qa_pairs, min(5, len(qa_pairs))):\n",
    "    print(f\"ğŸ“š {qa.get('category', 'N/A')}\")\n",
    "    print(f\"â“ {qa.get('question', 'N/A')}\")\n",
    "    ans = qa.get('answer', '')\n",
    "    print(f\"âœ… {ans[:250]}{'...' if len(ans) > 250 else ''}\")\n",
    "    print(f\"   Type: {qa.get('type')} | Difficulty: {qa.get('difficulty')}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = os.path.getsize(OUTPUT_FILE) / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ‰ Q&A GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“ Total Q&A pairs: {len(qa_pairs)}\")\n",
    "print(f\"ğŸ“ Output: {OUTPUT_FILE}\")\n",
    "print(f\"ğŸ’¾ Size: {file_size:.1f} MB\")\n",
    "print(f\"ğŸ“Š Categories: {len(cat_counts)}\")\n",
    "print(f\"âœ… Citation rate: {100*with_citations/len(qa_pairs):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nğŸ‘‰ Next: Run 03_raft_dataset_creation.ipynb\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}