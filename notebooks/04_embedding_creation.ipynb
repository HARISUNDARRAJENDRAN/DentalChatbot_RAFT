{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç FAISS Embedding Index Creation\n",
    "\n",
    "This notebook creates the FAISS vector index for retrieval during inference.\n",
    "\n",
    "**Input:** `chunks.jsonl` from notebook 01\n",
    "**Output:** `faiss_index/` folder with index and metadata\n",
    "\n",
    "**Embedding Model:** S-PubMedBert-MS-MARCO (medical domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/content/drive/MyDrive/RAFT_dental_data\"\n",
    "CHUNKS_FILE = f\"{DATA_DIR}/chunks.jsonl\"\n",
    "OUTPUT_DIR = f\"{DATA_DIR}/faiss_index\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(CHUNKS_FILE):\n",
    "    print(f\"‚úì Found chunks file\")\n",
    "else:\n",
    "    print(f\"‚úó chunks.jsonl not found. Run notebook 01 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load all chunks\n",
    "chunks = []\n",
    "with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"  ID: {chunks[0]['chunk_id']}\")\n",
    "print(f\"  Source: {chunks[0]['source']}\")\n",
    "print(f\"  Category: {chunks[0]['category']}\")\n",
    "print(f\"  Tokens: {chunks[0].get('token_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Medical domain embedding model\n",
    "MODEL_NAME = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "print(f\"Loading embedding model: {MODEL_NAME}\")\n",
    "embed_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "embedding_dim = embed_model.get_sentence_embedding_dimension()\n",
    "print(f\"‚úì Model loaded\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Max sequence length: {embed_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts\n",
    "chunk_texts = [c['text'] for c in chunks]\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts)} chunks...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = embed_model.encode(\n",
    "    chunk_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Generated embeddings\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Dtype: {embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "# Using IndexFlatIP (Inner Product) since embeddings are normalized -> cosine similarity\n",
    "print(\"Building FAISS index...\")\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(embeddings.astype(np.float32))\n",
    "\n",
    "print(f\"‚úì FAISS index built\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")\n",
    "print(f\"  Dimension: {index.d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the index with a sample query\n",
    "test_query = \"What are the indications for root canal treatment?\"\n",
    "\n",
    "# Encode query\n",
    "query_embedding = embed_model.encode([test_query], normalize_embeddings=True)\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "scores, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "    chunk = chunks[idx]\n",
    "    print(f\"\\n{i+1}. Score: {score:.4f}\")\n",
    "    print(f\"   Source: {chunk['source']}\")\n",
    "    print(f\"   Category: {chunk['category']}\")\n",
    "    print(f\"   Text: {chunk['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Index and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "index_path = os.path.join(OUTPUT_DIR, \"dental.index\")\n",
    "faiss.write_index(index, index_path)\n",
    "print(f\"‚úì Saved FAISS index to {index_path}\")\n",
    "\n",
    "# Save metadata (for retrieval)\n",
    "metadata = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    metadata.append({\n",
    "        \"id\": i,\n",
    "        \"chunk_id\": chunk['chunk_id'],\n",
    "        \"source\": chunk['source'],\n",
    "        \"category\": chunk['category'],\n",
    "        \"page_number\": chunk['page_number'],\n",
    "        \"text\": chunk['text']\n",
    "    })\n",
    "\n",
    "metadata_path = os.path.join(OUTPUT_DIR, \"metadata.jsonl\")\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    for m in metadata:\n",
    "        f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Saved metadata to {metadata_path}\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_vectors\": len(chunks),\n",
    "    \"index_type\": \"IndexFlatIP\",\n",
    "    \"normalized\": True\n",
    "}\n",
    "\n",
    "config_path = os.path.join(OUTPUT_DIR, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved config to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a retriever helper class\n",
    "retriever_code = '''\n",
    "\"\"\"\n",
    "Dental FAISS Retriever\n",
    "Load and use the FAISS index for retrieval.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DentalRetriever:\n",
    "    \"\"\"Retrieve relevant dental documents using FAISS.\"\"\"\n",
    "    \n",
    "    def __init__(self, index_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize retriever.\n",
    "        \n",
    "        Args:\n",
    "            index_dir: Directory containing FAISS index and metadata\n",
    "        \"\"\"\n",
    "        index_dir = Path(index_dir)\n",
    "        \n",
    "        # Load config\n",
    "        with open(index_dir / \"config.json\") as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(str(index_dir / \"dental.index\"))\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = []\n",
    "        with open(index_dir / \"metadata.jsonl\") as f:\n",
    "            for line in f:\n",
    "                self.metadata.append(json.loads(line))\n",
    "        \n",
    "        # Load embedding model\n",
    "        self.embed_model = SentenceTransformer(self.config[\"model_name\"])\n",
    "        \n",
    "        print(f\"Loaded retriever with {len(self.metadata)} documents\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of document dictionaries with scores\n",
    "        \"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embed_model.encode(\n",
    "            [query],\n",
    "            normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            doc = self.metadata[idx].copy()\n",
    "            doc[\"score\"] = float(score)\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    retriever = DentalRetriever(\"faiss_index\")\n",
    "    \n",
    "    results = retriever.retrieve(\"root canal indications\", k=3)\n",
    "    for r in results:\n",
    "        print(f\"Score: {r[\\'score\\']:.4f} | {r[\\'source\\']}\")\n",
    "'''\n",
    "\n",
    "retriever_path = os.path.join(OUTPUT_DIR, \"retriever.py\")\n",
    "with open(retriever_path, 'w') as f:\n",
    "    f.write(retriever_code)\n",
    "\n",
    "print(f\"‚úì Saved retriever helper to {retriever_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File sizes\n",
    "index_size = os.path.getsize(index_path) / (1024**2)\n",
    "metadata_size = os.path.getsize(metadata_path) / (1024**2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FAISS INDEX CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Total vectors: {index.ntotal}\")\n",
    "print(f\"üìê Embedding dimension: {embedding_dim}\")\n",
    "print(f\"üß† Model: {MODEL_NAME}\")\n",
    "print(f\"üíæ Index size: {index_size:.1f} MB\")\n",
    "print(f\"üìÑ Metadata size: {metadata_size:.1f} MB\")\n",
    "print(f\"üìÅ Output: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files created:\")\n",
    "print(f\"  - dental.index (FAISS index)\")\n",
    "print(f\"  - metadata.jsonl (document metadata)\")\n",
    "print(f\"  - config.json (index configuration)\")\n",
    "print(f\"  - retriever.py (helper class)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Run 05_model_training.ipynb for QLoRA fine-tuning\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Full Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are the steps for root canal treatment?\",\n",
    "    \"Explain the classification of dental caries\",\n",
    "    \"What are contraindications for dental implants?\",\n",
    "    \"How to manage dental emergencies?\"\n",
    "]\n",
    "\n",
    "print(\"Testing retrieval with sample queries:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Encode and search\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True).astype(np.float32)\n",
    "    scores, indices = index.search(q_emb, 3)\n",
    "    \n",
    "    print(\"Top 3 results:\")\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        chunk = chunks[idx]\n",
    "        print(f\"  [{score:.3f}] {chunk['source'][:40]}... (p.{chunk['page_number']})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
